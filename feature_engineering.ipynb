{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variable types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A variable is any characteristic, number or quantity that can be measured or counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Any kind of variable that can be represented by a number, these values can have two types <span class=\"burk\">Discrete</span> or <span class=\"burk\">Continuous</span>.\n",
    "\n",
    "<span class=\"burk\">Discrete</span>\n",
    "\n",
    "Any kind of numerical variable that is represented by whole numbers, usually belongs to the set of integers (Z).\n",
    "\n",
    "<span class=\"burk\">Continuous</span>\n",
    "\n",
    "Any kind of numerical variable that is represented by a range of values, usually belongs to the set of reals (R).\n",
    "\n",
    "There's other type of numerical variable that is also commom to feature in a dataset, <span class=\"burk\">Binary</span>. \n",
    "\n",
    "<span class=\"burk\">Binary</span>\n",
    "\n",
    "A numerical value with a number of possible values, usually used to represent categories through encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Any kind of value selected from a group of categories, also called labels. Can be separated by <span class=\"burk\">Ordinal</span> and <span class=\"burk\">Nominal</span> categories.\n",
    "\n",
    "<span class=\"burk\">Ordinal</span>\n",
    "\n",
    "Any kind of label representations that can be ordered in a meaningful way.\n",
    "\n",
    "<span class=\"burk\">Nominal</span>\n",
    "\n",
    "Any kind of label representation that doesn't carry a ordering or ranking by any means.\n",
    "\n",
    "Usually categorical variables are used in ML as encoded numerical variables so the algorithm can build a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Date and time variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A representation of the point in time the instance was extracted, can be separated by <span class=\"burk\">Date</span>, <span class=\"burk\">Time</span> and <span class=\"burk\">Date and Time</span>.\n",
    "\n",
    "It's a special case of categorical variable, although has a special treatment due to the enrichness of information it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Mixed variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some times real world features have both numbers and categories mixed together, these characteristics can be often <span class=\"burk\">numbers and labels among the same feature</span> or <span class=\"burk\">numbers and labels in the same instance of a feature</span>.\n",
    "\n",
    "This kind of feature usually carries a lot of information condensed and it requires a special treatment to retrieve informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset has its own characteristics, those are consequence of the characteristics of the features and how they relate to each other, all of these need to be adressed so its clear for the machine what is happening and the model be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span class=\"mark\">What is missing data?</span>\n",
    "\n",
    "Missing data, or missing values is a problem that occurs when a certain value is not stored for an instance or a variable. Is a very commom problem on datasets and have a big impact on conclusions derived from the data.\n",
    "\n",
    "The reasons a certain observation be missing can be <span class=\"burk\">Lost</span>, when the value is forgotten, lost or not stored properly, <span class=\"burk\">Don't exists</span>, commomly happens when a variable is result of a division and the result is simply not defined by the operation (dividing by 0, for instance) or <span class=\"burk\">Not found | Not identified</span>, when matching certain variables and for some reason any value is wrong or missing.\n",
    "\n",
    "<span class=\"mark\">Impacts of missing data</span>\n",
    "\n",
    "- Library incompatibility\n",
    "- Model performance modification\n",
    "- Imputation may distort data distribuition\n",
    "\n",
    "For imputing data into a dataset there is 3 main techniques as follows:\n",
    "\n",
    "- <span class=\"girk\">Missing Data Completely at Random (MCAR)</span>:\n",
    "    - Probability of being missing is the same for all the observations\n",
    "    - No relationship with other variables\n",
    "    - Disregarding those cases would not bias the inference made\n",
    "- <span class=\"girk\">Missing Data at Random (MAR)</span>:\n",
    "    - The values missing are somehow related to other information provided at the dataset\n",
    "- <span class=\"girk\">Missing Data not at Random (MNAR)</span>:\n",
    "    - There is a reason why some values are missing from the table\n",
    "\n",
    "To understand why data is missing is important to understand how data is collected, since is not always possible, is important to have the deepest knowledge of data collection so features can be best engineered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cardinality - Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When dealing with categorical variables, the number of labels a variable can assume is important for how that variable impact certain algorithms on training, the number of labels a variable can assume is called cardinality.\n",
    "\n",
    "Cardinality can play a big row on the outcome of a model such as:\n",
    "\n",
    "- Since categorical variables are commomly depicted as strings, the values need to be encoded\n",
    "    - Encoding techniques change the feture space and how the variables interact with each other\n",
    "- It can cause an uneven distrbuition due to high cardinality\n",
    "    - Labels may appear only on training datasets, leading to overfitting\n",
    "    - Labels may appear only on test datasets, model won't know how to treat values \n",
    "- Overffiting, mostly on tree based algorithms\n",
    "    - Variables with too many labels may dominate over other variables, mostly on entropy based algorithms\n",
    "    - High cardinality may introduce noise with little information\n",
    "- Operational problems\n",
    "    - Unseen labels may cause error since model doesn't know how to handle them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rare Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rare labels are also a frequent problem on datasets, usually, rare labels come from high cardinality variables, so the same problems of high cardinality are reflected.\n",
    "\n",
    "The real question on whether to keep or to change those labels relies on model performance, since its difficult to understand rare labels role on outcome predictions.\n",
    "\n",
    "Removing them and creating a new labels that groups all rare labels may be a way to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assumptions of linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some times, trying to fit a linear model to certain dataset is hard, for that matter is important to understand how features fit on the assumptions made by a linear fit, the information gathered by how each variable behaves is a good indicator for problems on model performance.\n",
    "\n",
    "- Linearity\n",
    "    - The mean values of the outcome variable for each increment of the predictors lie along a straight line.\n",
    "- No perfect multicolinearity\n",
    "    - No perfect linear relationship between two or more variables\n",
    "- Normal Distributed Errors\n",
    "    - The residual errors are random, normally distributed around a mean of 0\n",
    "- Homoscedaticity\n",
    "    - At any level of the predictor variables the variance of the residual terms should be zero\n",
    "\n",
    "When assumptions are not met, it means that features are not good enough to predict a certain outcome, for that matter there are three important problems face on assumptions:\n",
    "\n",
    "- Outliers\n",
    "- Lack of homoscedaticity\n",
    "- The variables are too skewed\n",
    "\n",
    "In order to overcome such problems there is some procedures that help surpass them, for outliers and lack of homoscedacity:\n",
    "\n",
    "- Mathematical transformations\n",
    "- Discretisation\n",
    "- Remove or censor outliers\n",
    "\n",
    "<span class=\"mark\">Evaluating model performance</span>\n",
    "\n",
    "There are some quantities that result from linear regression that helps to understand the quality of the fit.\n",
    "\n",
    "- Residual error analysis N(0,sigma)\n",
    "    - Since to perform linear regression the error term should be normalized and centered around zero, plotting such values must indicate the trend\n",
    "    - Normality can be statistically tested with Kolmorogov-Smirnov test for exemple\n",
    "- Homoscedasticity\n",
    "    - There are some tests and plots to determine homoscedasticity such as:\n",
    "        - Residuals plot\n",
    "        - Levene's test\n",
    "        - Barlett's test\n",
    "        - Goldfedt-Quandt test\n",
    "    - There are some visual evaluations that can be carried out, since this is the measure of distance of each point to the ideal linear model.\n",
    "- No Co-linearity\n",
    "    - This can be acessed by ploting the correlation matrix of all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Probability distribuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**What to do with non-linear models?**\n",
    "\n",
    "Sometimes a dataset can't be explained by a linear model, in this case, is required to work with a more broad approach and a distinct way of thinking about your data.\n",
    "\n",
    "**Probability Distribuitions**\n",
    "\n",
    "A function that gives the likelihood of a variable adopt a certain value, is a probability distribuition function, the probabilities must be between 0 and 1, the sum must of all probabilities is 1.\n",
    "\n",
    "In order to perform a Probability Distribuition taking care of the nature of the variable, there is a lot of methods that can be used to analyse the distribuition.\n",
    "\n",
    "- Discrete\n",
    "    - Binomial\n",
    "    - Poisson\n",
    "- Continuous\n",
    "    - Gaussian\n",
    "    - Skewed\n",
    "    - Many others\n",
    "\n",
    "**The Normal Distribuition**\n",
    "\n",
    "There is a few important features on a normal distribuition that can be useful to understand better non-linear variables, like all values are centered around a central peak, and the distance from the center dictates the probability of a certain value being adopted.\n",
    "\n",
    "**The Skewed Distribuition**\n",
    "\n",
    "When the tail of distribuition is longer than the other, there is the problem of skew, usually happens to unbalanced variable distribuitions. The main difference of a Skewed Distribuition is that the mean is affected by the skew, which means that not all values are centered around it. Usually, the mean sides with the longer tail.\n",
    "\n",
    "**Model Performance**\n",
    "\n",
    "The only algorithm that assumes that values are normally distributed is the linear regression, since all residual values must be normally distributed. Despite not considering normal distribuitions, models may have a improvement of performance normally distributed values.\n",
    "\n",
    "In order to create more normally distributed values, mathematical transformations and discretisation are attitudes that usually improves model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**What is an outlier?**\n",
    "\n",
    "An outlier is a value that is significantly different from other points, so much so, that lead to believe that the mechanism that created it is different from all the others.\n",
    "\n",
    "**Algorithms Suceptible to Outliers**\n",
    "\n",
    "- Linear Regression\n",
    "- AdaBoost\n",
    "\n",
    "**Identifing an Outlier**\n",
    "\n",
    "- For Normal Distribuitions\n",
    "99% of all observations are inside the range of mean more or less 3 standard deviations of distance around the center, hence, everything outside this scope is an outlier.\n",
    "\n",
    "- For Skewed Distribuitions\n",
    "Since Skewed Distribuitions aren't simmetrical, is necessary to use another means to calculate outliers, the best method is to use the IQR, Inter-Quantile Range, is is performed as follows:\n",
    "    \n",
    "    - IQR = 75º Quantile - 25º Quantile = 3º Quartile - 1º Quartile\n",
    "    - Upper Limit = 75º Quantile + IQR * 1.5 = 3º Quartile + IQR * 1.5\n",
    "    - Lower Limit = 25º Quantile - IQR * 1.5 = 1º Quartile - IQR * 1.5\n",
    "    \n",
    "To account extreme Outliers, the value must be multiplied by 3.\n",
    "\n",
    "**Visualizing Outliers**\n",
    "\n",
    "Boxplots are used to visualize outliers, it is important to note that, if a distribuition is skewed, the unbalanced nature shows how a value can be outliered by skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scales between variables are important to keep your eye on, for example, the coefficients on a linear regression can change values if inputs have different scales, in this case, larger scales will prevail upon smaller ones.\n",
    "\n",
    "Also, ranges play a big role on deciding on whether the coefficients are going to be heavier on output or not, this also needs to be adressed.\n",
    "\n",
    "Important parts of machine learning algorithms need to be carefully thought to be more performant, *gradient descent* converges faster with similar scales, *suport vectors* are found faster with scaled features and any *distance basde algorithm* is sensitive to magnitude.\n",
    "\n",
    "**Algorithms sensitive to magnitude**\n",
    "- Linear and Logistic Regression\n",
    "- Neural Networks\n",
    "- SVM\n",
    "- K-means clustering\n",
    "- KNN\n",
    "- LDA\n",
    "- PCA\n",
    "\n",
    "**Algorithms not sensitive to magnitude**\n",
    "- Classification and Regression Trees\n",
    "- Random Forests\n",
    "- Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How models deals with variables: a table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below resumes how models deals with troubles on your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T01:27:01.830906Z",
     "start_time": "2022-06-08T01:27:01.818375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"980\"\n",
       "            height=\"500\"\n",
       "            src=\"ML_Comparison.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1419a8e410>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"ML_Comparison.pdf\", width=980, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
